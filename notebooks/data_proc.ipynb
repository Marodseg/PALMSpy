{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisys PALMS Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:34:38.537121Z",
     "start_time": "2019-11-15T17:34:38.206033Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:35:17.295525Z",
     "start_time": "2019-11-15T17:35:17.289448Z"
    }
   },
   "outputs": [],
   "source": [
    "# select data directory\n",
    "\n",
    "work_dir = \"data/raw/NBBB baseline/\"\n",
    "work_dir = \"../test_data/\"\n",
    "acc = \"acc/\"  # accelerometer data (metadata)\n",
    "gps = \"gps/\"  # GPS data (csv format)\n",
    "merge = \"merge/\"\n",
    "\n",
    "# acc and GPS data files have the same name\n",
    "list_file_acc = sorted(os.listdir(work_dir + acc))\n",
    "list_file_gps = sorted(os.listdir(work_dir + gps))\n",
    "\n",
    "# select a file from the list\n",
    "file_gps = list_file_gps[0]\n",
    "file_acc = list_file_acc[0]\n",
    "file_merge = file_acc[:-4] + \"-merged.csv\"\n",
    "print(\"\\n Acc data file :   \" + file_acc)\n",
    "print(\" GPS data file :   \" + file_gps)\n",
    "print(\" Output file   :   \" + file_merge + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:11.018471Z",
     "start_time": "2019-11-15T17:36:03.003260Z"
    }
   },
   "outputs": [],
   "source": [
    "# read GPS data and create a dataframe\n",
    "\n",
    "path = work_dir + gps + file_gps\n",
    "\n",
    "columns = pd.read_csv(path, sep=',').columns.values.tolist() # extract headers\n",
    "columns = [columns[k].strip() for k in range(len(columns))] # remove initial/final spaces\n",
    "\n",
    "# remove last empty column\n",
    "gps_df = pd.read_csv(path, sep=',', names=columns).drop(columns=['INDEX'])\n",
    "\n",
    "# remove duplicate lines to get read of multiple header rows\n",
    "gps_df = gps_df.drop_duplicates()\n",
    "\n",
    "# remove the first row which contains a repetition of the header\n",
    "gps_df = gps_df.drop(gps_df.index[0])\n",
    "gps_df = gps_df.reset_index()\n",
    "gps_df = gps_df.drop(columns=['index'])\n",
    "\n",
    "try:\n",
    "    gps_df['UTC DATE'] = [gps_df['UTC DATE'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "    gps_df['UTC TIME'] = [gps_df['UTC TIME'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "    gps_df['LOCAL DATE'] = [gps_df['LOCAL DATE'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "    gps_df['LOCAL TIME'] = [gps_df['LOCAL TIME'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "    # add datetime column\n",
    "    gps_df['DATETIME'] = pd.to_datetime(gps_df['UTC DATE'] + \" \" + gps_df['UTC TIME'], format=\"%Y/%m/%d\")\n",
    "except:\n",
    "    try:\n",
    "        gps_df['DATE'] = [gps_df['DATE'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "        gps_df['TIME'] = [gps_df['TIME'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "        # add datetime column\n",
    "        gps_df['DATETIME'] = pd.to_datetime(gps_df['DATE'] + \" \" + gps_df['TIME'], format=\"%Y/%m/%d\")\n",
    "    except:\n",
    "        gps_df['UTC'] = [gps_df['UTC'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "        gps_df['LOCAL TIME'] = [gps_df['LOCAL TIME'][k].strip() for k in range(len(gps_df))] # remove initial/final spaces\n",
    "        # add datetime column\n",
    "        gps_df['DATETIME'] = pd.to_datetime(gps_df['UTC'], format=\"%Y/%m/%d\")\n",
    "\n",
    "print(gps_df.shape)\n",
    "#print(gps_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:12.501796Z",
     "start_time": "2019-11-15T17:36:11.228494Z"
    }
   },
   "outputs": [],
   "source": [
    "gps_df.describe(include='all').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:12.710736Z",
     "start_time": "2019-11-15T17:36:12.683830Z"
    }
   },
   "outputs": [],
   "source": [
    "gps_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:18.924852Z",
     "start_time": "2019-11-15T17:36:18.858893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read accelerometer data\n",
    "\n",
    "path = work_dir + acc + file_acc\n",
    "\n",
    "acc_list = [line.rstrip('\\n') for line in open(path)]\n",
    "\n",
    "acc_list[0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:21.217941Z",
     "start_time": "2019-11-15T17:36:21.211771Z"
    }
   },
   "outputs": [],
   "source": [
    "# print metadata\n",
    "\n",
    "start_time = acc_list[2].split()[2]\n",
    "start_date = acc_list[3].split()[2]\n",
    "interval = acc_list[4].split()[3]\n",
    "end_time = acc_list[5].split()[2]\n",
    "end_date = acc_list[6].split()[2]\n",
    "print(\"start time: %s\" % start_time)\n",
    "print(\"start date: %s\" % start_date)\n",
    "print(\"time interval: %s\" % interval)\n",
    "print(\"end time: %s\" % end_time)\n",
    "print(\"end date: %s\" % end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:23.137590Z",
     "start_time": "2019-11-15T17:36:23.116888Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    start_timestamp = dt.datetime.strptime(start_date + \" \" + start_time, '%d/%m/%Y %H:%M:%S')\n",
    "except:\n",
    "    try:\n",
    "        start_timestamp = dt.datetime.strptime(start_date + \" \" + start_time, '%m/%d/%Y %H:%M:%S')\n",
    "    except:\n",
    "        start_timestamp = dt.datetime.strptime(start_date + \" \" + start_time, '%Y/%m/%d %H:%M:%S')\n",
    "    \n",
    "x = time.strptime(interval, '%H:%M:%S')\n",
    "interval = dt.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec)\n",
    "try:\n",
    "    end_timestamp = dt.datetime.strptime(end_date + \" \" + end_time, '%d-%b-%y %H:%M:%S')\n",
    "except:\n",
    "    try:\n",
    "        end_timestamp = dt.datetime.strptime(end_date + \" \" + end_time, '%d-%m-%Y %H:%M:%S')\n",
    "    except:\n",
    "        try:\n",
    "            end_timestamp = dt.datetime.strptime(end_date + \" \" + end_time, '%d/%m/%Y %H:%M:%S')\n",
    "        except:\n",
    "            try:\n",
    "                end_timestamp = dt.datetime.strptime(end_date + \" \" + end_time, '%m/%d/%Y %H:%M:%S')\n",
    "            except:\n",
    "                end_timestamp = dt.datetime.strptime(end_date + \" \" + end_time, '%Y/%m/%d %H:%M:%S')\n",
    "\n",
    "if len(acc_list[10]) < 50: \n",
    "    acc_data = acc_list[10:]\n",
    "else:\n",
    "    acc_data = acc_list[11:]\n",
    "    \n",
    "tot_intervals = len(acc_data)\n",
    "print(\"start timestamp: %s\" % start_timestamp)\n",
    "print(\"end timestamp: %s\" % end_timestamp)\n",
    "print(\"interval in sec: %s\" % interval.total_seconds())\n",
    "print(\"number of acc. events: %d\" % tot_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:36:24.980625Z",
     "start_time": "2019-11-15T17:36:24.816837Z"
    }
   },
   "outputs": [],
   "source": [
    "# create accelerometer dataframe\n",
    "\n",
    "acc_df = pd.DataFrame({})\n",
    "acc_df['DATETIME'] = [start_timestamp + k*interval for k in range(tot_intervals)]\n",
    "acc_df['ACC DATA'] = acc_data\n",
    "acc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:37:10.561542Z",
     "start_time": "2019-11-15T17:36:26.875020Z"
    }
   },
   "outputs": [],
   "source": [
    "# select gps data based on accelerometer timestamps\n",
    "\n",
    "start_time = time.clock()\n",
    "w1 = np.zeros(len(gps_df))\n",
    "#w2 = np.zeros(len(gps_df))\n",
    "w3 = gps_df['DATETIME'].copy()\n",
    "for i in range(len(acc_df)):\n",
    "    w1 = abs((gps_df['DATETIME'] - acc_df['DATETIME'][i]).dt.total_seconds()) < interval.total_seconds()/2 # only one element equal to 1\n",
    "    #w2[w1] = 1  # select indices that match the condition in w1; NOT used\n",
    "    if w1.sum() == 1:  # found a value of gps_df which matches timestamp in acc_df\n",
    "        w3[w1] = acc_df['DATETIME'][i]\n",
    "            \n",
    "print(time.clock() - start_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T17:37:10.567453Z",
     "start_time": "2019-11-15T17:36:29.332Z"
    }
   },
   "outputs": [],
   "source": [
    "# create merged dataframe\n",
    "\n",
    "gps_df2 = gps_df.copy()\n",
    "gps_df2['DATETIME'] = w3\n",
    "merged_df = pd.merge(acc_df, gps_df2, on='DATETIME')\n",
    "merged_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write merged data frame on file\n",
    "try:\n",
    "    os.mkdir(work_dir + merge)\n",
    "    print(\"\\n Output directory: \" + work_dir + merge)\n",
    "except FileExistsError:\n",
    "    print(\"\\n Output directory: \" + work_dir + merge)\n",
    "\n",
    "\n",
    "path = work_dir + merge + file_merge\n",
    "merged_df.to_csv(path)\n",
    "print(\" Output file \" + file_merge + \" added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
